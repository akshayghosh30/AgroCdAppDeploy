name: '3-Destroy Apps from EKS'

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: 'AWS Region (e.g., us-east-1)'
        required: true
        default: 'us-east-1'
      cluster_name:
        description: 'EKS Cluster Name (e.g., staging-eks-demo)'
        required: true
        default: 'staging-eks-demo'

jobs:
  destroy-from-eks:
    name: 'Destroy ALB and Applications'
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.aws_region }}

#--------------------------- 1. Update kube-config-----------------------------------------




      - name: Update Kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ github.event.inputs.aws_region }} --name ${{ github.event.inputs.cluster_name }}
          echo "CORRECT:---> Kubeconfig updated for cluster: ${{ github.event.inputs.cluster_name }}"
#-------------------------------------------------------------------------------------------------------------------------------




# --------------------------2. Delete the ArgoCD Ingress to trigger ALB deletion------------------------------
# Checking if the ALB is deleted:
    # If deleted then, OUTPUT:"CORRECT:------> Success! The ALB is no longer found in AWS and is confirmed deleted."
    #If not deleted then, It will keep looping for 5 minutes , updating every 15 seconds
    # ALTERNATIVE SITUATION: There can be a possibility that the ALB is already deleted , then
        # OUTPUT : "------XXXX--- Could not find an ALB hostname from the Ingress. It might have been deleted already ---XXXX----------------"


      - name: Delete Ingress and Wait for ALB Deletion
        if: false
        run: |
          # Fetching ALB HOSTNAME 
          ALB_HOSTNAME=$(kubectl get ingress argocd-server-ingress -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")

          echo "****-------------------Deleting ArgoCD Ingress to trigger ALB deletion--------------****"
          kubectl delete -f kubernetes-manifests/argocd-ingress.yml -n argocd --ignore-not-found=true

          if [ -z "$ALB_HOSTNAME" ]; then
            echo "------XXXX--- Could not find an ALB hostname from the Ingress. It might have been deleted already ---XXXX----------------"
            echo "Skipping polling and proceeding."
          else
            echo "Actively polling to confirm deletion of ALB: $ALB_HOSTNAME"
            
            SECONDS=0
            TIMEOUT=300 # 5-minute timeout

            while [ $SECONDS -lt $TIMEOUT ]; do
              # Check if an ALB with the specific DNS name still exists
              ALB_ARN=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?DNSName=='$ALB_HOSTNAME'].LoadBalancerArn" --output text)
              
              if [ -z "$ALB_ARN" ]; then
                echo "CORRECT:------> Success! The ALB is no longer found in AWS and is confirmed deleted."
                break # Exit the loop successfully
              fi
              
              echo "****-------------------Waiting for ALB to be deleted... ($SECONDS/$TIMEOUT seconds)--------------****"
              sleep 15
              SECONDS=$((SECONDS + 15))
            done

            if [ $SECONDS -ge $TIMEOUT ]; then
              echo "ERROR: Timed out after 5 minutes waiting for ALB to be deleted."
              exit 1
            fi
          fi

          echo "Sleeping for 60 seconds to make sure that the Target Group and SGs deleted"
          sleep 60
#-------------------------------------------------------------------------------------------------------------------------------





#----------------------------------------- 3. Uninstall ArgoCD ---------------------------------------------------------------------


      - name: Uninstall ArgoCD
        run: |
          echo "xxxx-------------- Uninstalling ArgoCD Helm chart----------------X"
          helm uninstall argocd -n argocd || echo "ArgoCD not found, skipping."
#-------------------------------------------------------------------------------------------------------------------------------



#--------------------------------------- 4. Uninstall AWS Load Balancer Controller -----------------------------------------------



      - name: Uninstall AWS Load Balancer Controller
        run: |
          echo "xxxx---------Uninstalling AWS Load Balancer Controller Helm chart.----------xxxx"
          helm uninstall aws-load-balancer-controller -n kube-system || echo "AWS Load Balancer Controller not found, skipping."
#-------------------------------------------------------------------------------------------------------------------------------




#-------------------------------------  5. Detach IAM Policy from EKS Node Role ----------------------------------------
#NOTE: Why Detach over Delete
    # as it's a managed policy , it  could be used elsewhere



      - name: Detach IAM Policy for ALB Controller
        run: |
          echo "---------> Detaching IAM policy from node role..."
          POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='AWSLoadBalancerControllerIAMPolicy'].Arn" --output text)

          if [ -z "$POLICY_ARN" ]; then
            echo "IAM Policy 'AWSLoadBalancerControllerIAMPolicy' not found. Skipping detachment."
            exit 0
          fi

          NODE_GROUP_NAME=$(aws eks list-nodegroups --cluster-name ${{ github.event.inputs.cluster_name }} --query "nodegroups[0]" --output text)
          if [ -z "$NODE_GROUP_NAME" ]; then
            echo "No node groups found in cluster. Skipping detachment."
            exit 0
          fi

          NODE_ROLE_ARN=$(aws eks describe-nodegroup --cluster-name ${{ github.event.inputs.cluster_name }} --nodegroup-name "$NODE_GROUP_NAME" --query "nodegroup.nodeRole" --output text)
          NODE_ROLE_NAME=$(basename $NODE_ROLE_ARN)

          echo "Detaching policy $POLICY_ARN from role $NODE_ROLE_NAME"
          aws iam detach-role-policy --role-name $NODE_ROLE_NAME --policy-arn $POLICY_ARN || echo "Failed to detach policy, it might have been already detached."
          echo "COMPLETED:-------> Policy detachment process complete."
#-------------------------------------------------------------------------------------------------------------------------------


      - name: Completion Step
        run: |
          echo "=========================================================================================="
          echo "COMPLETED:-->>EKS cluster cleanup is complete."
          echo "You can now safely run '1-terraform-infra.yml' workflow to destroy the remaining infrastructure. Select 'destroy' as input during workflow dispatch"
          echo "=========================================================================================="
#-------------------------------------------------------------------------------------------------------------------------------
