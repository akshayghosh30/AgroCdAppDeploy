name: '3-Destroy Apps from EKS'

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: 'AWS Region (e.g., us-east-1)'
        required: true
        default: 'us-east-1'
      cluster_name:
        description: 'EKS Cluster Name (e.g., staging-eks-demo)'
        required: true
        default: 'staging-eks-demo'

jobs:
  destroy-from-eks:
    name: 'Destroy ALB Controller and ArgoCD'
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
#-------------------------------------------------------------------------------------------------------------------------------
#  Setup and AWS/Kubernetes Configuration

      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.aws_region }}

      - name: Install Tools (eksctl, helm)
        run: |
          echo "Installing eksctl..."
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          echo "eksctl version: $(eksctl version)"

      - name: Update Kubeconfig
        run: |
          echo "Connecting to EKS Cluster..."
          aws eks update-kubeconfig --region ${{ github.event.inputs.aws_region }} --name ${{ github.event.inputs.cluster_name }}
          echo "Kubeconfig updated. Verifying nodes:"
          kubectl get nodes

#-------------------------------------------------------------------------------------------------------------------------------
#  Step 1: Capture ALB Hostname
#  (Captures the ALB's DNS name to verify its deletion from AWS later)

      - name: 1. Capture ALB hostname before deletion
        id: capture_alb
        run: |
          echo "Capturing ALB Hostname..."
          ALB_HOSTNAME=$(kubectl get ingress argocd-server-ingress -n argcd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$ALB_HOSTNAME" ]; then
            echo "Found ALB hostname: $ALB_HOSTNAME"
            echo "alb_hostname=$ALB_HOSTNAME" >> "$GITHUB_OUTPUT"
          else
            echo "No ALB hostname found (may already be deleted)."
            echo "alb_hostname=" >> "$GITHUB_OUTPUT"
          fi
#-------------------------------------------------------------------------------------------------------------------------------





#-------------------------------------------------------------------------------------------------------------------------------
#  Step 2: Initiate Teardown  -NEW ADDITION  [EXPLICIT DELETION HERE]
#  (Deletes ArgoCD Apps and Ingress objects to trigger finalizers and start cleanup)
      - name: 2. Delete Applications and Ingresses (Initiate Teardown)
        run: |
          echo "Deleting ArgoCD Application resources..."
          kubectl delete app --all -n argocd --wait=false --ignore-not-found=true
          
          echo "Explicitly deleting Ingress resources to trigger ALB cleanup..."
          kubectl delete ingress --all -n argocd --ignore-not-found=true

          echo "Deleted argocd ingress in argocd namespace: SUCCESS"
          kubectl delete ingress --all -n default --ignore-not-found=true
          echo "Deleted default ingress in default namespace: SUCCESS"
          
          echo "Waiting 45 seconds for controllers to process deletions..."
          sleep 45

#-------------------------------------------------------------------------------------------------------------------------------



#-------------------------------------------------------------------------------------------------------------------------------
#  Step 3: Uninstall Helm Charts
#  (Removes the controllers themselves from the cluster)

      - name: 3. Uninstall Helm Charts (ArgoCD & ALB Controller)
        run: |
          echo "Uninstalling ArgoCD..."
          helm uninstall argocd -n argocd --wait || echo "ArgoCD not found, skipping."

          echo "Uninstalling AWS Load Balancer Controller..."
          helm uninstall aws-load-balancer-controller -n kube-system --wait || echo "ALB Controller not found, skipping."

#-------------------------------------------------------------------------------------------------------------------------------




#-------------------------------------------------------------------------------------------------------------------------------
#  Step 4: Wait for AWS Resource Deletion
#  (Pauses the workflow to confirm the physical ALB is gone from AWS)

      - name: 4. Wait for ALB to be deleted from AWS
        env:
          ALB_HOSTNAME: ${{ steps.capture_alb.outputs.alb_hostname }}
        run: |
          if [ -z "$ALB_HOSTNAME" ]; then
            echo "No ALB hostname captured, skipping wait."
            exit 0
          fi
          
          echo "Polling for ALB deletion: $ALB_HOSTNAME (Max wait: 3 minutes)"
          TIMEOUT=180
          ELAPSED=0
          while [ $ELAPSED -lt $TIMEOUT ]; do
            ALB_EXISTS=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?DNSName=='$ALB_HOSTNAME']" --output text 2>/dev/null)
            if [ -z "$ALB_EXISTS" ]; then
              echo "ALB has been deleted from AWS."
              exit 0
            fi
            echo "Waiting for ALB to be deleted... ($ELAPSED/$TIMEOUT seconds)"
            sleep 20
            ELAPSED=$((ELAPSED + 20))
          done
          echo "Timeout reached waiting for ALB deletion. Continuing cleanup..."

#-------------------------------------------------------------------------------------------------------------------------------





#-------------------------------------------------------------------------------------------------------------------------------
#  Step 5: Clean Up Kubernetes Resources - NEW ADDITION  [EXPLICIT DELETION HERE]
#  (Deletes namespaces and any leftover objects as a safety measure)

      - name: 5. Delete Namespaces and Remaining Kubernetes Objects
        run: |
          echo "Deleting argocd namespace..."
          kubectl delete namespace argocd --ignore-not-found=true --wait=true

          echo "Deleting remaining resources from default namespace as a safety measure..."
          kubectl delete deployment currency-converter -n default --ignore-not-found=true
          kubectl delete service currency-converter-service -n default --ignore-not-found=true
          kubectl delete secret api-keys -n default --ignore-not-found=true


#-------------------------------------------------------------------------------------------------------------------------------




#-------------------------------------------------------------------------------------------------------------------------------
#  Step 6: Delete IRSA
#  (Removes the IAM Role for Service Accounts association from the cluster)
      - name: 6. Delete IRSA using eksctl
        run: |
          echo "Deleting IRSA service account..."
          eksctl delete iamserviceaccount \
            --cluster=${{ github.event.inputs.cluster_name }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --region=${{ github.event.inputs.aws_region }} \
            --wait || echo "IRSA not found, skipping."

#-------------------------------------------------------------------------------------------------------------------------------




#-------------------------------------------------------------------------------------------------------------------------------
#  Step 7: Delete IAM Resources
#  (Deletes the underlying IAM Role and Policy from AWS)

      - name: 7. Delete IAM Role and Policy
        run: |
          ROLE_NAME="AmazonEKSLoadBalancerControllerRole-${{ github.event.inputs.cluster_name }}"
          POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
          
          echo "Attempting to delete IAM Role: $ROLE_NAME"
          POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='$POLICY_NAME'].Arn" --output text)
          
          if aws iam get-role --role-name "$ROLE_NAME" >/dev/null 2>&1; then
            if [ -n "$POLICY_ARN" ]; then
               aws iam detach-role-policy --role-name "$ROLE_NAME" --policy-arn "$POLICY_ARN" || true
            fi
            aws iam delete-role --role-name "$ROLE_NAME" || true
            echo "IAM Role deleted."
          else
            echo "IAM Role not found."
          fi
          
          echo "Attempting to delete IAM Policy: $POLICY_NAME"
          if [ -n "$POLICY_ARN" ]; then
            # This check prevents deleting a policy still attached to other roles
            ATTACHMENT_COUNT=$(aws iam get-policy --policy-arn $POLICY_ARN --query 'Policy.AttachmentCount' --output text)
            if [ "$ATTACHMENT_COUNT" -eq 0 ]; then
                aws iam delete-policy --policy-arn "$POLICY_ARN" || true
                echo "IAM Policy deleted."
            else
                echo "IAM Policy still has $ATTACHMENT_COUNT attachments, not deleting."
            fi
          else
            echo "IAM Policy not found."
          fi
#-------------------------------------------------------------------------------------------------------------------------------




#-------------------------------------------------------------------------------------------------------------------------------
#  Step 8: Final Confirmation
      - name: 8. Cleanup Complete
        run: |
          echo "CLEANUP COMPLETE. All cluster-level application resources have been removed."
          echo "You can now safely run 'terraform destroy'."
#-------------------------------------------------------------------------------------------------------------------------------
